{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyxlsb\n",
    "%pip install Unidecode\n",
    "%pip install gcsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "from collections import namedtuple\n",
    "from google.cloud import storage, bigquery\n",
    "from datetime import timedelta\n",
    "from functools import reduce\n",
    "from typing import List\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType\n",
    "from time import sleep\n",
    "import unidecode\n",
    "import re\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# CURRENT_DATE_ARG = \"2024-12-30T23:59:59\"\n",
    "CURRENT_DATE_ARG = None\n",
    "CURRENT_DATE_ARG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARENT_PROJECT = \"cloud-macro-tim\"\n",
    "BUCKET_NAME = \"tim-ultrafibra-gcs-sp\"\n",
    "ORIGINS=[\"ultrafibra-base-agendamento\"]\n",
    "CURRENT_DATE = datetime.strptime(CURRENT_DATE_ARG, '%Y-%m-%dT%H:%M:%S') if CURRENT_DATE_ARG is not None else datetime.today()\n",
    "DATASET_BY_ORIGIN = { \n",
    "  \"ultrafibra-base-agendamento\": \"disparos_agendamento\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BRAZILIAN_TIMEDIFF = timedelta(hours=3)\n",
    "CURRENT_DATE = CURRENT_DATE# - BRAZILIAN_TIMEDIFF\n",
    "CURRENT_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_midnight_hour(hour):\n",
    "  return hour >= 0 and hour <= 2\n",
    "\n",
    "if is_midnight_hour(CURRENT_DATE.hour):\n",
    "  CURRENT_DATE = CURRENT_DATE - timedelta(days=1)\n",
    "  CURRENT_DATE = CURRENT_DATE.replace(hour=23, minute=59, second=59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder.appName(\"raw_to_refined\")\\\n",
    "#   .config('parentProject', PARENT_PROJECT)\\\n",
    "#   .config(\"spark.sql.caseSensitive\", \"True\")\\\n",
    "#   .config('spark.sql.session.timeZone', 'America/Sao_Paulo')\\\n",
    "#   .config(\"spark.jars\", \"gs://spark-lib/bigquery/spark-3.3-bigquery-0.34.0.jar\")\\\n",
    "#   .getOrCreate()\n",
    "\n",
    "# gcsClient = storage.Client()\n",
    "# bqClient = bigquery.Client()\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/home/jovyan/work/Old/tim_ultrafibra/cloud-macro-tim-7291d4c1fbb9.json\"\n",
    "MAX_MEMORY = \"5g\"\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Ler Parquet do GCS\") \\\n",
    "    .config(\"spark.sql.caseSensitive\", \"True\")\\\n",
    "    .config(\"spark.sql.session.timeZone\", \"America/Sao_Paulo\")\\\n",
    "    .config(\"spark.network.timeout\", \"600s\")\\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"120s\")\\\n",
    "    .config(\"spark.jars\", \n",
    "            \"/usr/local/spark/jars/gcs-connector-hadoop3-latest.jar,\"\n",
    "            \"/usr/local/spark/jars/hadoop-common-3.3.6.jar,\"\n",
    "            \"/usr/local/spark/jars/hadoop-client-api-3.3.4.jar,\"\n",
    "            \"/usr/local/spark/jars/hadoop-client-runtime-3.3.4.jar\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]) \\\n",
    "    .config(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\n",
    "    .config(\"spark.jars\", \"gs://spark-lib/bigquery/spark-3.3-bigquery-0.34.0.jar\")\\\n",
    "    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "    .getOrCreate()\n",
    "gcsClient = storage.Client()\n",
    "bqClient = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_snake_case(input_string):\n",
    "    input_string = input_string.replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "    snake_case_string = re.sub(r'([a-z0-9])([A-Z])', r'\\1_\\2', input_string)\n",
    "    return snake_case_string.lower()\n",
    "\n",
    "def sanitize_column_name(text):\n",
    "  text = unidecode.unidecode(text)\n",
    "  text = to_snake_case(text).replace(\"/\", \"_\")\n",
    "  text = re.sub(r\"_\\d+(\\.\\d?)*_\", \"_\", text, count=1)\n",
    "  return text\n",
    "\n",
    "def rename_columns(df: DataFrame) -> DataFrame:\n",
    "  for col_name in df.columns:\n",
    "    df = df.withColumnRenamed(col_name, sanitize_column_name(col_name))\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_in_raw_zone(datetime: datetime, origin: str):\n",
    "  return f\"gs://{BUCKET_NAME}/raw-zone/{origin}/{datetime.strftime('%Y%m')}/{datetime.strftime('%Y%m%d-%H%M%S')}.csv\"\n",
    "\n",
    "def get_path_in_curated_zone(datetime: datetime, origin: str):\n",
    "  print(f\"gs://{BUCKET_NAME}/curated-zone/{origin}/{datetime.strftime('%Y%m')}/{datetime.strftime('%Y%m%d')}.parquet\")\n",
    "  return f\"gs://{BUCKET_NAME}/curated-zone/{origin}/{datetime.strftime('%Y%m')}/{datetime.strftime('%Y%m%d')}.parquet\"\n",
    "\n",
    "def get_path_in_curated_zone_backup(datetime: datetime, origin: str):\n",
    "  print(f\"gs://{BUCKET_NAME}/curated-zone/{origin}/{datetime.strftime('%Y%m')}/{datetime.strftime('%Y%m%d')}.parquet\")\n",
    "  return f\"gs://{BUCKET_NAME}/curated-zone/{origin}/backup/{datetime.strftime('%Y%m')}/{datetime.strftime('%Y%m%d')}.parquet\"\n",
    "\n",
    "def get_path_in_refined_zone(datetime: datetime, origin: str, table_name: str):\n",
    "  print(f\"gs://{BUCKET_NAME}/refined-zone/{origin}/{datetime.strftime('%Y%m')}/{table_name}.parquet\")\n",
    "  return f\"gs://{BUCKET_NAME}/refined-zone/{origin}/{datetime.strftime('%Y%m')}/{table_name}.parquet\"\n",
    "\n",
    "def get_prefix_in_raw_zone(datetime: datetime, origin: str):\n",
    "  print('raw zone')\n",
    "  print(f\"raw-zone/{origin}/{datetime.strftime('%Y%m')}/{datetime.strftime('%Y%m%d')}\")\n",
    "  return f\"raw-zone/{origin}/{datetime.strftime('%Y%m')}/{datetime.strftime('%Y%m%d')}\"\n",
    "\n",
    "def get_prefix_in_curated(datetime: datetime, origin: str):\n",
    "  return f\"curated-zone/{origin}/{datetime.strftime('%Y%m')}\"\n",
    "\n",
    "def get_prefix_backup_in_curated(origin: str):\n",
    "  return f\"curated-zone/{origin}/backup\"\n",
    "\n",
    "\n",
    "def get_prefix_in_refined(datetime: datetime, origin: str):\n",
    "  print('refined zone')\n",
    "  return f\"refined-zone/{origin}/{datetime.strftime('%Y%m')}\"\n",
    "\n",
    "def get_tmp_table_name(origin: str):\n",
    "  display(f'tim_ultrafibra_projeto_tct.{DATASET_BY_ORIGIN[origin]}_tmp')\n",
    "  return f'tim_ultrafibra_projeto_tct.{DATASET_BY_ORIGIN[origin]}_tmp'\n",
    "\n",
    "def get_full_table_name(origin: str):\n",
    "  display(f'tim_ultrafibra_projeto_tct.{DATASET_BY_ORIGIN[origin]}')\n",
    "  return f'tim_ultrafibra_projeto_tct.{DATASET_BY_ORIGIN[origin]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw to Curated\n",
    "\n",
    "Os dados são lidos da camada `raw` e carregados para a camada `curated` em formato parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = gcsClient.get_bucket(BUCKET_NAME)\n",
    "\n",
    "blobs_per_origin_in_raw = {}\n",
    "\n",
    "for origin in ORIGINS:\n",
    "  blobs = bucket.list_blobs(prefix=get_prefix_in_raw_zone(CURRENT_DATE, origin))\n",
    "  blobs_per_origin_in_raw[origin] = [f\"gs://{BUCKET_NAME}/{blob.name}\" for blob in blobs if blob.name.endswith(\".xlsb\") and f'{CURRENT_DATE.strftime(\"%Y%m%d\")}' in blob.name]\n",
    "  blobs_per_origin_in_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for origin_name, paths in blobs_per_origin_in_raw.items():\n",
    "  final_df = []\n",
    "  print(origin_name, paths)\n",
    "  for path in paths:\n",
    "    df = spark.createDataFrame(pd.read_excel(path, engine=\"pyxlsb\"))\n",
    "    final_df.append(rename_columns(df))\n",
    "  if len(final_df) > 1:\n",
    "    final_df = reduce(lambda df1, df2: df1.unionByName(df2, allowMissingColumns=True), final_df)\n",
    "    final_df.write.mode(\"overwrite\").parquet(get_path_in_curated_zone(CURRENT_DATE, origin_name))\n",
    "  elif len(final_df) == 1:\n",
    "    final_df[0].write.mode(\"overwrite\").parquet(get_path_in_curated_zone(CURRENT_DATE, origin_name))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curated to Refined\n",
    "\n",
    "Os blocos a seguir fazem a transformação dos dados curados para os dados refinados. A coluna `montante` é formatada e também removemos duplicatas utilizando as colunas `id_do_registro` e `hora_da_modificacao`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "blobs_per_origin_in_curated = {}\n",
    "\n",
    "for origin_name in ORIGINS:\n",
    "  blobs = gcsClient.list_blobs(BUCKET_NAME, prefix=get_prefix_in_curated(CURRENT_DATE, origin_name))\n",
    "  blobs_per_origin_in_curated[origin_name] = [f\"gs://{BUCKET_NAME}/{blob.name}\" for blob in blobs if blob.name.endswith(\".parquet/\") and CURRENT_DATE.strftime(\"%Y%m%d\") in blob.name]\n",
    "blobs_per_origin_in_curated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_origin_in_curated():\n",
    "    \"\"\"\n",
    "    Verifica se há arquivos `.parquet/` criados hoje dentro da estrutura curated para cada origem.\n",
    "    Retorna True se já houver backup (evitando novo disparo), e False caso contrário.\n",
    "    \"\"\"\n",
    "    blobs_per_origin_in_curated = {}\n",
    "\n",
    "    for origin_name in ORIGINS:\n",
    "        blobs = gcsClient.list_blobs(BUCKET_NAME, prefix=get_prefix_backup_in_curated(origin_name))\n",
    "        \n",
    "        # Filtrar apenas os arquivos da data atual\n",
    "        arquivos_hoje = [\n",
    "            f\"gs://{BUCKET_NAME}/backup/{blob.name}\" for blob in blobs\n",
    "            if blob.name.endswith(\".parquet/\") and f\"/{CURRENT_DATE.strftime('%Y%m%d')}.parquet/\" in blob.name\n",
    "        ]\n",
    "\n",
    "        blobs_per_origin_in_curated[origin_name] = arquivos_hoje\n",
    "\n",
    "    display(blobs_per_origin_in_curated)\n",
    "\n",
    "\n",
    "    return any(blobs_per_origin_in_curated.values())  # Se houver arquivos, retorna True, senão False\n",
    "\n",
    "validation = per_origin_in_curated()\n",
    "print(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dataframe(dfs: list):\n",
    "  if len(dfs) == 0:\n",
    "    return spark.createDataFrame([], StringType())\n",
    "  elif len(dfs) == 1:\n",
    "    return dfs[0]\n",
    "  else:\n",
    "    return reduce(lambda df1, df2: df1.unionByName(df2, allowMissingColumns=True), dfs)\n",
    "\n",
    "def save_dataframe(df: DataFrame, origin: str, table_name: str):\n",
    "  df.write.format(\"parquet\").mode(\"overwrite\").save(get_path_in_refined_zone(CURRENT_DATE,origin, table_name))\n",
    "\n",
    "def save_dataframe_buckup(df: DataFrame, origin: str):\n",
    "  df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(get_path_in_curated_zone_backup(CURRENT_DATE,origin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_columns = {\n",
    "        'dt_agendamento': 'data_agendamento',\n",
    "        'hora': 'hora_processada'\n",
    "}\n",
    "\n",
    "valid_columns = [\"ordem\", \"uf\", 'dt_agendamento', 'status', 'hora']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_df_vazio():\n",
    "    \"\"\"Cria um DataFrame vazio com a estrutura esperada.\"\"\"\n",
    "    schema = StructType([\n",
    "        StructField(\"dt_agendamento\", DateType(), True),\n",
    "        StructField(\"ordem\", StringType(), True),\n",
    "        StructField(\"uf\", StringType(), True),\n",
    "        StructField(\"status\", StringType(), True),\n",
    "        StructField(\"hora\", StringType(), True),\n",
    "    ])\n",
    "    \n",
    "    return spark.createDataFrame([], schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def read_file_xlsb():\n",
    "    for origin in ORIGINS:\n",
    "        dfs = []\n",
    "        \n",
    "        all_ufs = []\n",
    "        \n",
    "        paths = blobs_per_origin_in_curated.get(origin, [])\n",
    "        print(paths)\n",
    "        if not paths:\n",
    "            print(f\"⚠️ Nenhum arquivo encontrado\")\n",
    "            # df_filtrado = criar_df_vazio()\n",
    "            # all_ufs = []\n",
    "            return None, None\n",
    "\n",
    "        print(\"🚀 Arquivos Encontrados. Processando dataframe.\")\n",
    "        for path in paths:\n",
    "            print(f\"📂 Processando arquivo: {path}\")\n",
    "            df = spark.read.parquet(path)\n",
    "            display(df.show())\n",
    "            dfs.append(df)\n",
    "        if dfs:\n",
    "            reduced_df = reduce_dataframe(dfs)\n",
    "\n",
    "            # all_ufs = reduced_df.select(\"uf\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "            all_ufs = (reduced_df.select(F.upper(\"uf\").alias(\"uf\")).distinct().rdd.flatMap(lambda x: x).collect())\n",
    "            df_spark = (reduced_df.withColumn(\"uf\", F.upper(F.col(\"uf\"))).withColumn(\"dt_agendamento\", F.expr(\"date_add('1899-12-30', CAST(dt_agendamento AS INT))\").cast(DateType())))\n",
    "            df_spark = df_spark.dropDuplicates()\n",
    "            # hoje = F.date_sub(F.current_date(), 0)\n",
    "            # hoje = F.lit(\"2025-02-19\").cast(\"date\")\n",
    "            hoje = F.current_date()\n",
    "\n",
    "            df_filtrado = df_spark.filter(F.col(\"dt_agendamento\") == hoje)\n",
    "            # display(df_filtrado.show())\n",
    "            return df_filtrado, all_ufs\n",
    "\n",
    "def send_request(ordens:List[str]):\n",
    "\n",
    "    if validation:\n",
    "        print('⚠️ Backup encontrado. Cancelando disparos...')\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    print(\"🚀 Nenhum backup encontrado. Iniciando o disparo de mensagens...\")\n",
    "    \n",
    "    url = \"https://www.zohoapis.com/crm/v2/functions/enviarmensagemativabyordernumber/actions/execute?auth_type=apikey&zapikey=1003.a887d07e81c67367e2bd9ded02383d2a.b03cc99897ccf7d0558373c6e68a5cd7\"\n",
    "\n",
    "    data = { \"data\" : {\n",
    "        \"CellPhoneNumber\": None,\n",
    "        \"Orders\": ordens,\n",
    "        \"ChatBotStateId\": \"8bff0e4b-c2bc-4ecf-a221-b7f2a4136b69\",\n",
    "        \"ChatBotId\": \"timliveinicio\",\n",
    "        \"ChatBotMessageNamespace\": \"842a11f072ac_4f6d_b67d_6fc599c69b8b\",\n",
    "        \"ChatBotRouterKey\": \"Key dGltbW92ZWxyb3V0ZXI6azJ3Mk1GU3RWT211eXZYNXZQVXU=\",\n",
    "        \"ContactExtrasVariables\": {\n",
    "        \"crmId\": \"49cd0467-a7ec-42b7-8ac5-8fed77fa29f9\",\n",
    "        \"cpf\": \"36610602875\"\n",
    "        },\n",
    "        \"ChatBotMessageName\": \"tim_tct_agendamento\",\n",
    "        \"chatBotMessageNoPayload\": None,\n",
    "        \"ChatBotResource\": \"9c945f28-6433-4de6-9f1e-fb5515c12549\",\n",
    "        \"ChatBotMessageYesPayload\": None\n",
    "    }}\n",
    "    # print('fazendo req...')\n",
    "    # print(data)\n",
    "    files = {'arguments': (None, json.dumps(data))}\n",
    "    response = requests.request(\"POST\", url, files=files)\n",
    "    # display(response.status_code)\n",
    "    # display(response.text)\n",
    "\n",
    "    json_data = response.text\n",
    "    # display(json_data)\n",
    "    # json_data = '{\"Orders\": {\"Success\": [\"1-1655903095358\", \"1-1656651105927\", \"1-1656456309233\", \"1-1656704421755\", \"1-1656659017704\", \"1-1656390140197\", \"1-1656717187506\", \"1-1656297674826\", \"1-1656722750560\", \"1-1656725407194\", \"1-1656055282650\", \"1-1656739869671\", \"1-1656729946669\", \"1-1656732535438\", \"1-1656734026035\", \"1-1656738957776\", \"1-1656746612835\", \"1-1656748924932\", \"1-1656768745691\", \"1-1656776094545\", \"1-1656502450956\", \"1-1656538785705\", \"1-1656544603429\", \"1-1656553468349\", \"1-1656571926645\", \"1-1656570302313\", \"1-1656579637540\", \"1-1656589700651\", \"1-1656596594580\", \"1-1656612638654\", \"1-1656615709385\", \"1-1656645689915\", \"1-1656632885225\", \"1-1656642717110\", \"1-1656642172484\", \"1-1656641477325\", \"1-1656643844684\"], \"Failure\": []}, \"Result\": \"success\"}'\n",
    "\n",
    "    return json_data\n",
    "\n",
    "def gerar_df(json_data, df_spark:DataFrame):\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    data = json.loads(json_data)\n",
    "    df = pd.DataFrame({\n",
    "        \"ordem\": data[\"Orders\"][\"Success\"],\n",
    "        \"status\": \"Success\",\n",
    "    })\n",
    "    if data[\"Orders\"][\"Failure\"]:\n",
    "        df_failure = pd.DataFrame({\n",
    "            \"ordem\": data[\"Orders\"][\"Failure\"],\n",
    "            \"status\": \"Failure\"})\n",
    "        df = pd.concat([df, df_failure], ignore_index=True)\n",
    "\n",
    "\n",
    "    df_spark_data = spark.createDataFrame(df)\n",
    "    df_spark = df_spark.filter(F.col(\"ordem\").isin(df_spark_data.select(\"ordem\").rdd.flatMap(lambda x: x).collect()))\n",
    "\n",
    "    final_dataframe = df_spark.join(df_spark_data, on=\"ordem\", how=\"inner\")\n",
    "\n",
    "    \n",
    "    display(final_dataframe.toPandas())\n",
    "    # final_dataframe = df_spark.join(df_spark_data, on=\"ordem\", how=\"inner\")\n",
    "    # display(final_dataframe.toPandas())\n",
    "    # all_ufs.append(final_dataframe)\n",
    "\n",
    "    # dataframe = reduce_dataframe(all_ufs)\n",
    "\n",
    "    return final_dataframe\n",
    "\n",
    "def dataframe_pr(df_spark:DataFrame):\n",
    "    df_filtrado = df_spark.filter((F.col(\"uf\") == \"PR\"))\n",
    "    ordens_pr = df_filtrado.select(\"ordem\").rdd.flatMap(lambda x: x).collect()\n",
    "    df_ordenado = df_filtrado.orderBy(\"ordem\")\n",
    "    display(len(ordens_pr))\n",
    "    # display(df_ordenado.toPandas())\n",
    "    ordens_divididas_pr = chunk_list(ordens_pr)\n",
    "    # display(ordens_divididas_pr)\n",
    "    return df_ordenado, ordens_divididas_pr\n",
    "\n",
    "def dataframe_all_ufs(df_spark:DataFrame):\n",
    "    '''\n",
    "    Disparo geral sem PR.\n",
    "    '''\n",
    "\n",
    "    # ufs = [\"SP\", \"RJ\", \"MG\"]\n",
    "    ufs = df_spark.select(\"uf\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "    display(ufs)\n",
    "    df_filtrado = df_spark.filter(~(F.col(\"uf\") == \"PR\") & (F.col(\"uf\").isin(ufs)))\n",
    "    ordens = df_filtrado.select(\"ordem\").rdd.flatMap(lambda x: x).collect()\n",
    "    print(len(ordens))\n",
    "    final_df_all = df_filtrado.orderBy(\"ordem\")\n",
    "    ordens_divididas = chunk_list(ordens)\n",
    "    display(len(ordens_divididas))\n",
    "    return ordens_divididas, final_df_all\n",
    "\n",
    "def chunk_list(lst, chunk_size=50):\n",
    "    \"\"\"Divide uma lista em sublistas de tamanho máximo especificado.\"\"\"\n",
    "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark, all_ufs = read_file_xlsb()\n",
    "\n",
    "if df_spark != None:\n",
    "    display(df_spark.show())\n",
    "\n",
    "all_dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_dados_pr(df_spark):\n",
    "    if df_spark is None:\n",
    "        print(\"⚠️ Arquivo .xlsb indisponível!\")\n",
    "        return criar_df_vazio()  \n",
    "        \n",
    "    print(\"✅ Executando fluxo.\")\n",
    "    only_pr = []\n",
    "    hora_execucao = (datetime.now() - timedelta(hours=3)).strftime(\"%H:%M:%S\") # Captura a hora dentro da função\n",
    "    final_df_all, ordens_divididas = dataframe_pr(df_spark)\n",
    "    final_df_all.show()\n",
    "\n",
    "    for ordem in ordens_divididas:\n",
    "        \n",
    "        print(ordem)\n",
    "        json_data = send_request(ordens=ordem)\n",
    "        \n",
    "        if json_data is None:  \n",
    "            print(\"⚠️ Json Vazio.\")\n",
    "            final_df_all = criar_df_vazio()\n",
    "            final_df_all.show()\n",
    "            return final_df_all\n",
    "\n",
    "        dfs = gerar_df(json_data=json_data, df_spark=final_df_all)\n",
    "        \n",
    "\n",
    "        only_pr.append(dfs)\n",
    "\n",
    "    final_df_pr = reduce_dataframe(only_pr)\n",
    "    final_df_pr = final_df_pr.select(*[F.col(col) for col in valid_columns if col in final_df_pr.columns])\n",
    "    final_df_pr = final_df_pr.withColumn(\"hora\", F.lit(hora_execucao))\n",
    "    final_df_pr.show()\n",
    "    all_dfs.append(final_df_pr)\n",
    "    \n",
    "    return final_df_pr \n",
    "\n",
    "final_df_pr = processar_dados_pr(df_spark)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def processar_dados_ufs(df_spark):\n",
    "    if df_spark is None:\n",
    "        print(\"⚠️ Arquivo .xlsb indisponível!\")\n",
    "        return criar_df_vazio()  # Retorna um DataFrame vazio\n",
    "    print(\"Esperando 3min para o disparo...\")\n",
    "    sleep(180)\n",
    "    print(\"✅ Executando fluxo.\")\n",
    "    all_ufs = []\n",
    "    ordens_divididas, final_df_all = dataframe_all_ufs(df_spark)\n",
    "    display(ordens_divididas)\n",
    "    print(final_df_all.show())\n",
    "    for ordem in ordens_divididas:\n",
    "        hora_execucao = (datetime.now() - timedelta(hours=3)).strftime(\"%H:%M:%S\")\n",
    "        hora_execucao  # Captura a hora dentro da função\n",
    "        print(ordem)\n",
    "        json_data = send_request(ordens=ordem)\n",
    "\n",
    "        if json_data is None:  # Se não houver dados, retorna DF vazio\n",
    "            print(\"⚠️ Json Vazio.\")\n",
    "            final_df_all = criar_df_vazio()\n",
    "            final_df_all.show()\n",
    "            return final_df_all\n",
    "\n",
    "        dfs = gerar_df(json_data=json_data, df_spark=final_df_all)\n",
    "        dfs = dfs.withColumn(\"hora\", F.lit(hora_execucao))\n",
    "        all_ufs.append(dfs)\n",
    "    \n",
    "    # Reduzindo os DataFrames\n",
    "    final_df_all = reduce_dataframe(all_ufs)\n",
    "    final_df_all = final_df_all.select(*[F.col(col) for col in valid_columns if col in final_df_all.columns])\n",
    "    final_df_all.show()\n",
    "\n",
    "    all_dfs.append(final_df_all)\n",
    "    \n",
    "    return final_df_all\n",
    "\n",
    "\n",
    "df_final = processar_dados_ufs(df_spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_df = reduce_dataframe(all_dfs)\n",
    "merged_df = merged_df.withColumn(\"dt_agendamento\", F.col(\"dt_agendamento\").cast(StringType()))\n",
    "print(merged_df.printSchema())\n",
    "for old_col, new_col in rename_columns.items():\n",
    "    print(old_col, new_col)\n",
    "    if old_col in merged_df.columns:  \n",
    "        merged_df = merged_df.withColumnRenamed(old_col, new_col)\n",
    "\n",
    "merged_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if merged_df.count() > 0: #salvar disparos\n",
    "    save_dataframe_buckup(df=merged_df, origin=origin)\n",
    "\n",
    "if merged_df.count() > 0: #salvar df main\n",
    "    save_dataframe(df=merged_df, origin=origin, table_name=\"agendamentos_tct\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refined to Bigquery\n",
    "\n",
    "Os blocos a seguir são responsáveis por fazer a carga dos dados refinados para o Bigquery.\n",
    "\n",
    "Primeiro são lidos os arquivos da camada refined referentes ao mês armazenado na variável `CURRENT_DATE`.\n",
    "\n",
    "Para cada arquivo lido, temos um dataframe que remove as duplicidades a partir das colunas `id_do_registro` e `hora_da_modificacao`. Em seguida, é feita a carga para o Bigquery utilizando o método `save_on_bigquery`.\n",
    "\n",
    "O método `save_on_bigquery` recebe como parâmetro o dataframe, o nome da origem dos dados, a lista de colunas que devem ser salvos no bigquery e a lista de colunas que serão usadas para fazer o merge dos dados novos com os existentes no BigQuery. Neste método, se a tabela não existir, ela é criada. Caso contrário, os dados são adicionados a uma tabela temporária, depois são feitos os merges com a tabela existente e então a tabela temporária é excluída.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs_per_origin_in_refined = {}\n",
    "\n",
    "for origin_name in ORIGINS:\n",
    "  blobs = gcsClient.list_blobs(BUCKET_NAME, prefix=get_prefix_in_refined(CURRENT_DATE, origin_name))\n",
    "  blobs_per_origin_in_refined[origin_name] = [f\"gs://{BUCKET_NAME}/{blob.name}\" for blob in blobs if blob.name.endswith(\".parquet/\")]\n",
    "blobs_per_origin_in_refined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JoinOnField = namedtuple('JoinOnField', ['field_name', 'isNullable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_on_bigquery(df: DataFrame, origin: str, fields: list, joinOn: List[JoinOnField]):\n",
    "  tmp_table_name = get_tmp_table_name(origin)\n",
    "  display(tmp_table_name)\n",
    "  full_table_name = get_full_table_name(origin)\n",
    "  display(full_table_name)\n",
    "  table = None\n",
    "\n",
    "  try:\n",
    "    table = bqClient.get_table(full_table_name)\n",
    "  except Exception:\n",
    "    table = None\n",
    "  \n",
    "  if table is None:\n",
    "    df.write.format('bigquery')\\\n",
    "      .option(\"temporaryGcsBucket\", BUCKET_NAME)\\\n",
    "      .option(\"table\", full_table_name)\\\n",
    "      .option(\"parentProject\", PARENT_PROJECT)\\\n",
    "      .mode(\"overwrite\")\\\n",
    "      .save()\n",
    "  else:\n",
    "    bqClient.query(f\"DROP TABLE IF EXISTS {tmp_table_name}\")\n",
    "\n",
    "    df.write.format(\"bigquery\")\\\n",
    "      .option(\"temporaryGcsBucket\", BUCKET_NAME)\\\n",
    "      .option(\"table\", tmp_table_name)\\\n",
    "      .option(\"parentProject\", PARENT_PROJECT)\\\n",
    "      .mode(\"append\")\\\n",
    "      .save()\n",
    "\n",
    "    select = \", \".join(fields)\n",
    "    joinCondition = \" AND \".join([\n",
    "      f\"(COALESCE(target.`{field.field_name}`, \\\"\\\") = COALESCE(origin.`{field.field_name}`, \\\"\\\"))\" if field.isNullable \n",
    "      else f\"target.`{field.field_name}` = origin.`{field.field_name}`\" \n",
    "      for field in joinOn\n",
    "    ])\n",
    "    update = \", \".join([f\"target.`{field}` = origin.`{field}`\" for field in fields])\n",
    "    insertFields = \", \".join(fields)\n",
    "    insertValues = \", \".join([f\"origin.`{field}`\" for field in fields])\n",
    "\n",
    "    query = f\"\"\"\n",
    "        MERGE INTO {full_table_name} AS target\n",
    "        USING (SELECT {select} FROM {tmp_table_name}) AS origin\n",
    "        ON {joinCondition}\n",
    "        WHEN MATCHED THEN \n",
    "          UPDATE SET {update}\n",
    "        WHEN NOT MATCHED THEN\n",
    "          INSERT ({insertFields})\n",
    "          VALUES ({insertValues})\n",
    "      \"\"\"\n",
    "    bqClient.query(query).result()\n",
    "    print(\"Saved on BigQuery: \", full_table_name)\n",
    "    bqClient.query(f\"DROP TABLE {tmp_table_name}\")\n",
    "\n",
    "  pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_columns = ['data_agendamento', 'ordem', 'uf', 'status', 'processed_at', 'hora_processada']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if validation == False and merged_df.count() > 0:\n",
    "    print('⏳ Gravando no BQ.')\n",
    "    for origin in DATASET_BY_ORIGIN:\n",
    "        # print(origin)\n",
    "        df_deals = spark.read.parquet(get_path_in_refined_zone(CURRENT_DATE, origin, \"agendamentos_tct\"))\n",
    "        print(df_deals.show())\n",
    "        df_deals = df_deals.select([column for column in df_deals.columns if column in valid_columns])\n",
    "\n",
    "        drop_duplicates_by = [\"data_agendamento\", \"ordem\"]\n",
    "        join_fields_by = [\n",
    "            JoinOnField(\"data_agendamento\", False),\n",
    "            JoinOnField(\"ordem\", True),\n",
    "            # JoinOnField(\"nome_produto\", True),\n",
    "        ]\n",
    "\n",
    "        if df_deals.count() > 0:\n",
    "            save_on_bigquery(\n",
    "            df_deals.dropDuplicates(drop_duplicates_by).where(F.col(\"ordem\").isNotNull()), \n",
    "            origin,\n",
    "            list(df_deals.columns), \n",
    "            join_fields_by)\n",
    "else:\n",
    "    print('❌ Arquivo Vazio. Cancelando save_on_bigquery()')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "8a94588eda9d64d9e9a351ab8144e55b1fabf5113b54e67dd26a8c27df0381b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
